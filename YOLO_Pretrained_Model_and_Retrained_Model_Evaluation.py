# -*- coding: utf-8 -*-
"""YOLO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H8etbaAQpZmTlWSEhhrhRRaLuU4VyLA3
"""

!pip install ultralytics
!pip install -U ipywidgets



from google.colab import drive
drive.mount('/content/gdrive/')
import os
import random

from ultralytics import YOLO
import cv2

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from PIL import Image

def load_labels(image_file, train_labels):
    label_file = os.path.splitext(image_file)[0] + ".txt"
    label_path = os.path.join(train_labels, label_file)

    with open(label_path, "r") as f:
        labels = f.read().strip().split("\n")

    return labels

current_path = os.getcwd()
print("Current working directory:", current_path)
entries = os.listdir('.')
print(entries)

def plot_object_detections(ax, image, labels):
    for label in labels:
        if len(label.split()) != 5:
            continue
        class_id, x_center, y_center, width, height = map(float, label.split())
        x_min = int((x_center - width/2) * image.shape[1])
        y_min = int((y_center - height/2) * image.shape[0])
        x_max = int((x_center + width/2) * image.shape[1])
        y_max = int((y_center + height/2) * image.shape[0])
        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 255, 0), 3)

    ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    ax.axis('off')

# Set paths
train_images = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/train/images"
train_labels = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/train/labels"


image_files = os.listdir(train_images)
random_images = random.sample(image_files, 8)

fig, axs = plt.subplots(2, 4, figsize=(12, 4))

for i, image_file in enumerate(random_images):
    row, col = divmod(i, 4)

    image_path = os.path.join(train_images, image_file)
    image = cv2.imread(image_path)

    labels = load_labels(image_file, train_labels)

    plot_object_detections(axs[row, col], image, labels)

plt.show()

"""# Pre-*trained*"""

model = YOLO("yolov8x.pt")


# Define the directory containing images
images_directory = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/train/images"
image_files = [os.path.join(images_directory, file) for file in os.listdir(images_directory) if file.endswith('.jpg')]

for image_path in image_files:
    results = model(image_path)

for r in results:
  plot = r.plot()
  plot = cv2.cvtColor(plot, cv2.COLOR_BGR2RGB)
  display(Image.fromarray(plot))

# Define the label map according to the COCO dataset
COCO_map = {
    0: 'person',        1: 'bicycle',       2: 'car',           3: 'motorcycle',    4: 'airplane',
    5: 'bus',           6: 'train',         7: 'truck',         8: 'boat',          9: 'traffic light',
    10: 'fire hydrant', 11: 'stop sign',    12: 'parking meter',13: 'bench',        14: 'bird',
    15: 'cat',          16: 'dog',          17: 'horse',        18: 'sheep',        19: 'cow',
    20: 'elephant',     21: 'bear',         22: 'zebra',        23: 'giraffe',      24: 'backpack',
    25: 'umbrella',     26: 'handbag',      27: 'tie',          28: 'suitcase',     29: 'frisbee',
    30: 'skis',         31: 'snowboard',    32: 'sports ball',  33: 'kite',         34: 'baseball bat',
    35: 'baseball glove',36: 'skateboard',  37: 'surfboard',    38: 'tennis racket',39: 'bottle',
    40: 'wine glass',   41: 'cup',          42: 'fork',         43: 'knife',        44: 'spoon',
    45: 'bowl',         46: 'banana',       47: 'apple',        48: 'sandwich',     49: 'orange',
    50: 'broccoli',     51: 'carrot',       52: 'hot dog',      53: 'pizza',        54: 'donut',
    55: 'cake',         56: 'chair',        57: 'couch',        58: 'potted plant', 59: 'bed',
    60: 'dining table', 61: 'toilet',       62: 'tv',           63: 'laptop',       64: 'mouse',
    65: 'remote',       66: 'keyboard',     67: 'cell phone',   68: 'microwave',    69: 'oven',
    70: 'toaster',      71: 'sink',         72: 'refrigerator', 73: 'book',         74: 'clock',
    75: 'vase',         76: 'scissors',     77: 'teddy bear',   78: 'hair drier',   79: 'toothbrush'
}


traffic_map_2 = {
    0: 'truck', 1: 'truck', 2: 'motorcycle', 3: 'bicycle', 4: 'bus', 5: 'car',
    6: 'truck', 7: 'human hauler', 8: 'bus', 9: 'truck', 10: 'motorcycle',
    11: 'truck', 12: 'car', 13: 'rickshaw', 14: 'scooter', 15: 'car',
    16: 'car', 17: 'three wheelers (CNG)', 18: 'truck', 19: 'truck', 20: 'wheelbarrow'
}

# Desired classes to display in COCO prediction
desired_classes = {'bicycle', 'car', 'motorcycle', 'bus', 'train', 'truck'}

import torch
import torchvision.transforms.functional as F
import numpy as np
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.patches as patches

def load_labels(label_file, image_width, image_height):
    """Load labels from a text file in YOLO format"""
    labels = []
    with open(label_file, 'r') as file:
        for line in file:
            parts = line.split()
            # Convert YOLO format to xmin, ymin, xmax, ymax
            x_center = float(parts[1]) * image_width
            y_center = float(parts[2]) * image_height
            width = float(parts[3]) * image_width
            height = float(parts[4]) * image_height
            xmin = x_center - width / 2
            ymin = y_center - height / 2
            xmax = x_center + width / 2
            ymax = y_center + height / 2

            label = {
                'class': int(parts[0]),
                'xmin': xmin,
                'ymin': ymin,
                'xmax': xmax,
                'ymax': ymax
            }
            labels.append(label)
    return labels

def convert_prediction(prediction):
    """Convert model prediction to a list of dictionaries"""
    pred_boxes = prediction.boxes.xyxy.cpu().numpy()
    pred_scores = prediction.boxes.conf.cpu().numpy()
    pred_labels = prediction.boxes.cls.cpu().numpy()

    predictions_list = []
    for box, score, label in zip(pred_boxes, pred_scores, pred_labels):
        class_name = COCO_map[label] if label in COCO_map else str(label)
        if class_name in desired_classes:
            prediction_dict = {
                'box': box.tolist(),
                'score': score,
                'label': int(label)
            }
            predictions_list.append(prediction_dict)

    return predictions_list

def calculate_iou(boxA, boxB):
    """Calculate the Intersection over Union (IoU) of two bounding boxes."""
    # Extract coordinates from boxes
    boxA_xmin, boxA_ymin, boxA_xmax, boxA_ymax = boxA[0], boxA[1], boxA[2], boxA[3]
    boxB_xmin, boxB_ymin, boxB_xmax, boxB_ymax = boxB['xmin'], boxB['ymin'], boxB['xmax'], boxB['ymax']

    # determine the (x, y)-coordinates of the intersection rectangle
    xA = max(boxA_xmin, boxB_xmin)
    yA = max(boxA_ymin, boxB_ymin)
    xB = min(boxA_xmax, boxB_xmax)
    yB = min(boxA_ymax, boxB_ymax)

    # compute the area of intersection rectangle
    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)

    # compute the area of both the prediction and ground-truth rectangles
    boxAArea = (boxA_xmax - boxA_xmin + 1) * (boxA_ymax - boxA_ymin + 1)
    boxBArea = (boxB_xmax - boxB_xmin + 1) * (boxB_ymax - boxB_ymin + 1)

    # compute the intersection over union by taking the intersection area
    # and dividing it by the sum of prediction + ground-truth areas - the intersection area
    iou = interArea / float(boxAArea + boxBArea - interArea)

    # return the intersection over union value
    return iou

def calculate_ap(precision, recall):
    """Calculate Average Precision (AP) given precision and recall arrays."""
    # Append 0 and 1 to recall and precision arrays to ensure they start at 0 and end at 1
    recall = np.concatenate((np.array([0.]), recall, np.array([1.])))
    precision = np.concatenate((np.array([0.]), precision, np.array([0.])))

    # Compute the area under the precision-recall curve
    ap = np.trapz(precision, recall)

    return ap


def calculate_mAP(predictions, labels, label_map=None):
    """Calculate mean Average Precision (mAP) at different IoU thresholds (mAP50-95)."""
    # Initialize arrays to store precision and recall values at different IoU thresholds
    all_precisions = []
    all_recalls = []

    # Iterate over different IoU thresholds from 0.5 to 0.95 with a step size of 0.05
    iou_thresholds = np.arange(0.5, 1.0, 0.05)
    for threshold in iou_thresholds:
        # Calculate precision and recall for the current IoU threshold
        precision, recall = calculate_metrics(predictions, labels, threshold, label_map)
        all_precisions.append(precision)
        all_recalls.append(recall)

    # Calculate AP for each class
    ap_values = []
    for precision, recall in zip(all_precisions, all_recalls):
        # Ensure recall is an array
        recall = np.array([recall]) if isinstance(recall, float) else recall
        precision = np.array([precision]) if isinstance(precision, float) else precision

        # Append 0 and 1 to recall and precision arrays to ensure they start at 0 and end at 1
        recall = np.concatenate(([0.], recall, [1.]))
        precision = np.concatenate(([0.], precision, [0.]))

        # Compute the area under the precision-recall curve
        ap = np.trapz(precision, recall)
        ap_values.append(ap)

    # Calculate mean Average Precision (mAP) across all classes
    mAP = np.mean(ap_values)

    return mAP

def calculate_metrics(predictions, labels, threshold=0.5, label_map=None):
    """Calculate precision, recall, and mAP"""
    true_positives = 0
    false_positives = 0
    false_negatives = 0
    all_ground_truth = len(labels)

    for pred in predictions:
        if pred['score'] >= threshold:
            found_match = False
            pred_class_name = COCO_map[pred['label']] if label_map is not None else str(pred['label'])
            for label in labels:
                true_class_name = traffic_map_2[label['class']] if label_map is not None else str(label['class'])
                if pred_class_name == true_class_name:
                    iou = calculate_iou(pred['box'], label)
                    if iou >= 0.5:
                        true_positives += 1
                        found_match = True
                        break
            if not found_match:
                false_positives += 1

    false_negatives = all_ground_truth - true_positives

    if true_positives + false_positives == 0:
        precision = np.array([0])  # Set precision to zero if both true positives and false positives are zero
    else:
        precision = np.array([true_positives / (true_positives + false_positives)])  # Convert to array

    if true_positives + false_negatives == 0:
        recall = np.array([0])  # Set recall to zero if both true positives and false negatives are zero
    else:
        recall = np.array([true_positives / (true_positives + false_negatives)])  # Convert to array

    return precision, recall

def visualize_comparison(image, predictions, labels, coco_map, traffic_map):
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    # Plot predicted bounding boxes
    axes[0].imshow(image)
    axes[0].set_title('Predicted')
    for pred in predictions:
        box = pred['box']
        xmin, ymin, xmax, ymax = box[0], box[1], box[2], box[3]
        width = xmax - xmin
        height = ymax - ymin
        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor='r', facecolor='none')
        axes[0].add_patch(rect)
        class_label = coco_map[pred['label']]
        axes[0].text(xmin, ymin, class_label, color='r', fontsize=8)

    # Plot corrected labeled bounding boxes
    axes[1].imshow(image)
    axes[1].set_title('Corrected Label')
    for label in labels:
        xmin, ymin, xmax, ymax = label['xmin'], label['ymin'], label['xmax'], label['ymax']
        width = xmax - xmin
        height = ymax - ymin
        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor='g', facecolor='none')
        axes[1].add_patch(rect)
        class_label = traffic_map_2[label['class']]
        axes[1].text(xmin, ymin, class_label, color='g', fontsize=8)

    plt.show()

# Example usage


# Set paths
test_images = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/images"
test_labels = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/labels"

# Load an image and its labels
random_image = random.choice(os.listdir(test_images))
image_path = os.path.join(test_images, random_image)
image = Image.open(image_path).convert("RGB")
label_path = os.path.join(test_labels, random_image.replace(".jpg", ".txt"))

image_cmp = image
label_path_cmp = label_path

# Get image dimensions
image_width, image_height = image.size

# Load labels
ground_truth = load_labels(label_path, image_width, image_height)

# Perform the prediction
mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = YOLO("yolov8x.pt")
results = model(image)
#model.eval()
for result in results:
  with torch.no_grad():
      prediction = convert_prediction(result)

# Calculate metrics using traffic_map for labels
precision, recall = calculate_metrics(prediction, ground_truth, label_map=traffic_map_2)
mAP = calculate_mAP(prediction, ground_truth, label_map=traffic_map_2)
print("Precision:", precision)
print("Recall:", recall)
print("mAP50-95:", mAP)

# Visualize the predictions and corrected labeled bounding boxes with class labels
visualize_comparison(image, prediction, ground_truth, COCO_map, traffic_map_2)

import os
import random
import torch
import torchvision.transforms.functional as F
from PIL import Image
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from tqdm import tqdm

traffic_map = {
    0: 'ambulance', 1: 'army vehicle', 2: 'auto rickshaw', 3: 'bicycle', 4: 'bus', 5: 'car',
    6: 'garbage van', 7: 'human hauler', 8: 'minibus', 9: 'minivan', 10: 'motorbike',
    11: 'pickup', 12: 'police car', 13: 'rickshaw', 14: 'scooter', 15: 'suv',
    16: 'taxi', 17: 'three wheelers (CNG)', 18: 'truck', 19: 'van', 20: 'wheelbarrow'
}

# Set paths
test_images = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/images"
test_labels = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/labels"

# Initialize lists to store metrics
all_precisions = []
all_recalls = []
all_mAPs = []

# Iterate over all images in the test folder
num_images = len(os.listdir(test_images))
for image_name in tqdm(os.listdir(test_images), total=num_images):
    # Load image
    image_path = os.path.join(test_images, image_name)
    image = Image.open(image_path).convert("RGB")

    # Load labels
    label_path = os.path.join(test_labels, image_name.replace(".jpg", ".txt"))
    image_width, image_height = image.size
    ground_truth = load_labels(label_path, image_width, image_height)

    # Perform the prediction
    mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = YOLO("yolov8x.pt")
    results = model(image)
    #model.eval()
    for result in results:
      with torch.no_grad():
          prediction = convert_prediction(result)

    # Calculate metrics
    precision, recall = calculate_metrics(prediction, ground_truth, label_map=traffic_map)
    mAP = calculate_mAP(prediction, ground_truth, label_map=traffic_map)
    # print("Precision:", precision)
    # print("Recall:", recall)
    # print("mAP50-95:", mAP)

    # Append metrics to lists
    all_precisions.append(precision)
    all_recalls.append(recall)
    all_mAPs.append(mAP)

# Compute mean metrics
mean_precision = sum(all_precisions) / len(all_precisions)
mean_recall = sum(all_recalls) / len(all_recalls)
mean_mAP = sum(all_mAPs) / len(all_mAPs)


print("Mean Precision:", mean_precision)
print("Mean Recall:", mean_recall)
print("Mean mAP50-95:", mean_mAP)

model = YOLO('yolov8x.yaml')
model.load('yolov8x.pt')
model.train(data = '/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/data_1.yaml',
            epochs = 30,
            imgsz = 640,
            seed = 77,
            batch = 8,
            device=0,
            name = 'traffic_detection_50',
            resume = True,
            plots = True)



model = YOLO('yolov8x.yaml')
model.load('yolov8x.pt')
model.train(data = '/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/data_1.yaml',
            epochs = 50,
            imgsz = 640,
            seed = 39,
            batch = 16,
            device=0,
            name = 'traffic_detection_50_3',
            resume = True,
            plots = True)

trained_map = {
    0: 'ambulance', 1: 'army vehicle', 2: 'auto rickshaw', 3: 'bicycle', 4: 'bus', 5: 'car',
    6: 'garbage van', 7: 'human hauler', 8: 'minibus', 9: 'minivan', 10: 'motorbike',
    11: 'pickup', 12: 'police car', 13: 'rickshaw', 14: 'scooter', 15: 'suv',
    16: 'taxi', 17: 'three wheelers (CNG)', 18: 'truck', 19: 'van', 20: 'wheelbarrow'
}

import torch
import torchvision.transforms.functional as F
import numpy as np
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from sklearn.metrics import precision_recall_curve, average_precision_score
import matplotlib.patches as patches

def load_labels(label_file, image_width, image_height):
    """Load labels from a text file in YOLO format"""
    labels = []
    with open(label_file, 'r') as file:
        for line in file:
            parts = line.split()
            # Convert YOLO format to xmin, ymin, xmax, ymax
            x_center = float(parts[1]) * image_width
            y_center = float(parts[2]) * image_height
            width = float(parts[3]) * image_width
            height = float(parts[4]) * image_height
            xmin = x_center - width / 2
            ymin = y_center - height / 2
            xmax = x_center + width / 2
            ymax = y_center + height / 2

            label = {
                'class': int(parts[0]),
                'xmin': xmin,
                'ymin': ymin,
                'xmax': xmax,
                'ymax': ymax
            }
            labels.append(label)
    return labels

def convert_prediction(prediction):
    """Convert model prediction to a list of dictionaries"""
    pred_boxes = prediction.boxes.xyxy.cpu().numpy()
    pred_scores = prediction.boxes.conf.cpu().numpy()
    pred_labels = prediction.boxes.cls.cpu().numpy()

    predictions_list = []
    for box, score, label in zip(pred_boxes, pred_scores, pred_labels):
        class_name = trained_map[label] if label in trained_map else str(label)
        if class_name in desired_classes:
            prediction_dict = {
                'box': box.tolist(),
                'score': score,
                'label': int(label)
            }
            predictions_list.append(prediction_dict)

    return predictions_list

def calculate_iou(boxA, boxB):
    """Calculate the Intersection over Union (IoU) of two bounding boxes."""
    # Extract coordinates from boxes
    boxA_xmin, boxA_ymin, boxA_xmax, boxA_ymax = boxA[0], boxA[1], boxA[2], boxA[3]
    boxB_xmin, boxB_ymin, boxB_xmax, boxB_ymax = boxB['xmin'], boxB['ymin'], boxB['xmax'], boxB['ymax']

    # determine the (x, y)-coordinates of the intersection rectangle
    xA = max(boxA_xmin, boxB_xmin)
    yA = max(boxA_ymin, boxB_ymin)
    xB = min(boxA_xmax, boxB_xmax)
    yB = min(boxA_ymax, boxB_ymax)

    # compute the area of intersection rectangle
    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)

    # compute the area of both the prediction and ground-truth rectangles
    boxAArea = (boxA_xmax - boxA_xmin + 1) * (boxA_ymax - boxA_ymin + 1)
    boxBArea = (boxB_xmax - boxB_xmin + 1) * (boxB_ymax - boxB_ymin + 1)

    # compute the intersection over union by taking the intersection area
    # and dividing it by the sum of prediction + ground-truth areas - the intersection area
    iou = interArea / float(boxAArea + boxBArea - interArea)

    # return the intersection over union value
    return iou

def calculate_ap(precision, recall):
    """Calculate Average Precision (AP) given precision and recall arrays."""
    # Append 0 and 1 to recall and precision arrays to ensure they start at 0 and end at 1
    recall = np.concatenate((np.array([0.]), recall, np.array([1.])))
    precision = np.concatenate((np.array([0.]), precision, np.array([0.])))
    #precision = np.concatenate((np.array([1.]), precision, np.array([precision[-1]])))

    # Compute the area under the precision-recall curve
    ap = np.trapz(precision, recall)

    return ap


def calculate_mAP(predictions, labels, label_map=None):
    """Calculate mean Average Precision (mAP) at different IoU thresholds (mAP50-95)."""
    # Initialize arrays to store precision and recall values at different IoU thresholds
    all_precisions = []
    all_recalls = []

    # Iterate over different IoU thresholds from 0.5 to 0.95 with a step size of 0.05
    iou_thresholds = np.arange(0.5, 1.0, 0.05)
    for threshold in iou_thresholds:
        # Calculate precision and recall for the current IoU threshold
        precision, recall = calculate_metrics(predictions, labels, threshold, label_map)
        all_precisions.append(precision)
        all_recalls.append(recall)

    # Calculate AP for each class
    ap_values = []
    for precision, recall in zip(all_precisions, all_recalls):
        # Ensure recall is an array
        recall = np.array([recall]) if isinstance(recall, float) else recall
        precision = np.array([precision]) if isinstance(precision, float) else precision

        # Append 0 and 1 to recall and precision arrays to ensure they start at 0 and end at 1
        recall = np.concatenate(([0.], recall, [1.]))
        precision = np.concatenate(([0.], precision, [0.]))

        # Compute the area under the precision-recall curve
        ap = np.trapz(precision, recall)
        ap_values.append(ap)

    # Calculate mean Average Precision (mAP) across all classes
    mAP = np.mean(ap_values)

    return mAP

def calculate_metrics(predictions, labels, threshold=0.5, label_map=None):
    """Calculate precision, recall, and mAP"""
    true_positives = 0
    false_positives = 0
    false_negatives = 0
    all_ground_truth = len(labels)

    for pred in predictions:
        if pred['score'] >= threshold:
            found_match = False
            pred_class_name = trained_map[pred['label']] if label_map is not None else str(pred['label'])
            for label in labels:
                true_class_name = traffic_map_2[label['class']] if label_map is not None else str(label['class'])
                if pred_class_name == true_class_name:
                    iou = calculate_iou(pred['box'], label)
                    if iou >= 0.5:
                        true_positives += 1
                        found_match = True
                        break
            if not found_match:
                false_positives += 1

    false_negatives = all_ground_truth - true_positives

    if true_positives + false_positives == 0:
        precision = np.array([0])  # Set precision to zero if both true positives and false positives are zero
    else:
        precision = np.array([true_positives / (true_positives + false_positives)])  # Convert to array

    if true_positives + false_negatives == 0:
        recall = np.array([0])  # Set recall to zero if both true positives and false negatives are zero
    else:
        recall = np.array([true_positives / (true_positives + false_negatives)])  # Convert to array

    return precision, recall

def visualize_comparison(image, predictions, labels, trained_map, traffic_map):
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    # Plot predicted bounding boxes
    axes[0].imshow(image)
    axes[0].set_title('Predicted')
    for pred in predictions:
        box = pred['box']
        xmin, ymin, xmax, ymax = box[0], box[1], box[2], box[3]
        width = xmax - xmin
        height = ymax - ymin
        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor='r', facecolor='none')
        axes[0].add_patch(rect)
        class_label = trained_map[pred['label']]
        axes[0].text(xmin, ymin, class_label, color='r', fontsize=8)

    # Plot corrected labeled bounding boxes
    axes[1].imshow(image)
    axes[1].set_title('Corrected Label')
    for label in labels:
        xmin, ymin, xmax, ymax = label['xmin'], label['ymin'], label['xmax'], label['ymax']
        width = xmax - xmin
        height = ymax - ymin
        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor='g', facecolor='none')
        axes[1].add_patch(rect)
        class_label = traffic_map[label['class']]
        axes[1].text(xmin, ymin, class_label, color='g', fontsize=8)

    plt.show()

# Set paths
test_images = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/images"
test_labels = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/labels"

# Load an image and its labels
# random_image = random.choice(os.listdir(test_images))
# image_path = os.path.join(test_images, random_image)
# image = Image.open(image_path).convert("RGB")
# label_path = os.path.join(test_labels, random_image.replace(".jpg", ".txt"))




# Get image dimensions
image_width, image_height = image.size

# Load labels
ground_truth = load_labels(label_path, image_width, image_height)

# Perform the prediction
mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_path = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trained/yolov8_8_30.pt"
model = YOLO(model_path)
results = model(image)
#model.eval()
for result in results:
  with torch.no_grad():
      prediction = convert_prediction(result)

# Calculate metrics using traffic_map for labels
precision, recall = calculate_metrics(prediction, ground_truth, label_map=traffic_map_2)
mAP = calculate_mAP(prediction, ground_truth, label_map=traffic_map_2)
print("Precision:", precision)
print("Recall:", recall)
print("mAP50-95:", mAP)

# Visualize the predictions and corrected labeled bounding boxes with class labels
visualize_comparison(image, prediction, ground_truth, trained_map, traffic_map_2)

import os
import random
import torch
import torchvision.transforms.functional as F
from PIL import Image
from torchvision.models.detection import fasterrcnn_resnet50_fpn
from tqdm import tqdm

traffic_map = {
    0: 'ambulance', 1: 'army vehicle', 2: 'auto rickshaw', 3: 'bicycle', 4: 'bus', 5: 'car',
    6: 'garbage van', 7: 'human hauler', 8: 'minibus', 9: 'minivan', 10: 'motorbike',
    11: 'pickup', 12: 'police car', 13: 'rickshaw', 14: 'scooter', 15: 'suv',
    16: 'taxi', 17: 'three wheelers (CNG)', 18: 'truck', 19: 'van', 20: 'wheelbarrow'
}

# Set paths
test_images = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/images"
test_labels = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/labels"

# Initialize lists to store metrics
all_precisions = []
all_recalls = []
all_mAPs = []

# Iterate over all images in the test folder
num_images = len(os.listdir(test_images))
for image_name in tqdm(os.listdir(test_images), total=num_images):
    # Load image
    image_path = os.path.join(test_images, image_name)
    image = Image.open(image_path).convert("RGB")

    # Load labels
    label_path = os.path.join(test_labels, image_name.replace(".jpg", ".txt"))
    image_width, image_height = image.size
    ground_truth = load_labels(label_path, image_width, image_height)

    # Perform the prediction
    mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model_path = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trained/yolov8_8_30.pt"
    model = YOLO(model_path)
    results = model(image)
    #model.eval()
    for result in results:
      with torch.no_grad():
          prediction = convert_prediction(result)

    # Calculate metrics
    precision, recall = calculate_metrics(prediction, ground_truth, label_map=traffic_map)
    mAP = calculate_mAP(prediction, ground_truth, label_map=traffic_map)
    # print("Precision:", precision)
    # print("Recall:", recall)
    # print("mAP50-95:", mAP)

    # Append metrics to lists
    all_precisions.append(precision)
    all_recalls.append(recall)
    all_mAPs.append(mAP)

# Compute mean metrics
mean_precision = sum(all_precisions) / len(all_precisions)
mean_recall = sum(all_recalls) / len(all_recalls)
mean_mAP = sum(all_mAPs) / len(all_mAPs)


print("Mean Precision:", mean_precision)
print("Mean Recall:", mean_recall)
print("Mean mAP50-95:", mean_mAP)

def convert_prediction(prediction):
    """Convert model prediction to a list of dictionaries"""
    pred_boxes = prediction.boxes.xyxy.cpu().numpy()
    pred_scores = prediction.boxes.conf.cpu().numpy()
    pred_labels = prediction.boxes.cls.cpu().numpy()

    predictions_list = []
    for box, score, label in zip(pred_boxes, pred_scores, pred_labels):
        class_name = trained_map[label] if label in trained_map else str(label)
        prediction_dict = {
            'box': box.tolist(),
            'score': score,
            'label': int(label)
        }
        predictions_list.append(prediction_dict)

    return predictions_list

# Set paths
test_images = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/images"
test_labels = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/labels"

# Initialize lists to store metrics
all_precisions = []
all_recalls = []
all_mAPs = []

# Iterate over all images in the test folder
num_images = len(os.listdir(test_images))
for image_name in tqdm(os.listdir(test_images), total=num_images):
    # Load image
    image_path = os.path.join(test_images, image_name)
    image = Image.open(image_path).convert("RGB")

    # Load labels
    label_path = os.path.join(test_labels, image_name.replace(".jpg", ".txt"))
    image_width, image_height = image.size
    ground_truth = load_labels(label_path, image_width, image_height)

    # Perform the prediction
    mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model_path = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trained/yolov8_8_30.pt"
    model = YOLO(model_path)
    results = model(image)
    #model.eval()
    for result in results:
      with torch.no_grad():
          prediction = convert_prediction(result)

    # Calculate metrics
    precision, recall = calculate_metrics(prediction, ground_truth, label_map=traffic_map)
    mAP = calculate_mAP(prediction, ground_truth, label_map=traffic_map)
    # print("Precision:", precision)
    # print("Recall:", recall)
    # print("mAP50-95:", mAP)

    # Append metrics to lists
    all_precisions.append(precision)
    all_recalls.append(recall)
    all_mAPs.append(mAP)

# Compute mean metrics
mean_precision = sum(all_precisions) / len(all_precisions)
mean_recall = sum(all_recalls) / len(all_recalls)
mean_mAP = sum(all_mAPs) / len(all_mAPs)


print("Mean Precision:", mean_precision)
print("Mean Recall:", mean_recall)
print("Mean mAP50-95:", mean_mAP)



def convert_prediction(prediction):
    """Convert model prediction to a list of dictionaries"""
    pred_boxes = prediction.boxes.xyxy.cpu().numpy()
    pred_scores = prediction.boxes.conf.cpu().numpy()
    pred_labels = prediction.boxes.cls.cpu().numpy()

    predictions_list = []
    for box, score, label in zip(pred_boxes, pred_scores, pred_labels):
        class_name = trained_map[label] if label in trained_map else str(label)
        prediction_dict = {
            'box': box.tolist(),
            'score': score,
            'label': int(label)
        }
        predictions_list.append(prediction_dict)

    return predictions_list

def visualize_comparison(image, predictions, labels, trained_map, traffic_map):
    fig, axes = plt.subplots(1, 2, figsize=(12, 6))

    # Plot predicted bounding boxes
    axes[0].imshow(image)
    axes[0].set_title('Predicted')
    for pred in predictions:
        box = pred['box']
        xmin, ymin, xmax, ymax = box[0], box[1], box[2], box[3]
        width = xmax - xmin
        height = ymax - ymin
        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor='r', facecolor='none')
        axes[0].add_patch(rect)
        class_label = trained_map[pred['label']]
        axes[0].text(xmin, ymin, class_label, color='r', fontsize=8)

    # Plot corrected labeled bounding boxes
    axes[1].imshow(image)
    axes[1].set_title('Corrected Label')
    for label in labels:
        xmin, ymin, xmax, ymax = label['xmin'], label['ymin'], label['xmax'], label['ymax']
        width = xmax - xmin
        height = ymax - ymin
        rect = patches.Rectangle((xmin, ymin), width, height, linewidth=1, edgecolor='g', facecolor='none')
        axes[1].add_patch(rect)
        class_label = traffic_map[label['class']]
        axes[1].text(xmin, ymin, class_label, color='g', fontsize=8)

    plt.show()

# Set paths
test_images = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/images"
test_labels = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trafic_data/test/labels"

# Load an image and its labels
random_image = random.choice(os.listdir(test_images))
image_path = os.path.join(test_images, random_image)
image = Image.open(image_path).convert("RGB")
label_path = os.path.join(test_labels, random_image.replace(".jpg", ".txt"))


# Get image dimensions
image_width, image_height = image.size

# Load labels
ground_truth = load_labels(label_path, image_width, image_height)

# Perform the prediction
mydevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model_path = "/content/gdrive/My Drive/Colab Notebooks/CS543_Proj/trained/yolov8_8_30.pt"
model = YOLO(model_path)
results = model(image)
#model.eval()
for result in results:
  with torch.no_grad():
      prediction = convert_prediction(result)

# Calculate metrics using traffic_map for labels
precision, recall = calculate_metrics(prediction, ground_truth, label_map=traffic_map_2)
mAP = calculate_mAP(prediction, ground_truth, label_map=traffic_map_2)
print("Precision:", precision)
print("Recall:", recall)
print("mAP50-95:", mAP)

# Visualize the predictions and corrected labeled bounding boxes with class labels
visualize_comparison(image, prediction, ground_truth, trained_map, traffic_map)

